# Deep Learning Chronicles

Welcome to **Deep Learning Chronicles**! This repository is dedicated to summarizing and providing insights into important research papers in the field of deep learning. Each paper is linked with a short summary and a blog post for an in-depth understanding.

## Table of Contents

| No. | Paper Name                                                                 | Paper Link                                                 | Year | Short Summary                                                                                                                                                               | Summary Readme.md | Blog Post                                                 |
|-----|----------------------------------------------------------------------------|------------------------------------------------------------|------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------|-----------------------------------------------------------|
| 1   | 3D Convolutional Neural Networks for Human Action Recognition             | [Paper Link](https://www.dbs.ifi.lmu.de/~yu_k/icml2010_3dcnn.pdf) | 2010 | A study on using 3D CNNs for recognizing human actions in video sequences.                                                                                            |                   |                                                           |
| 2   | NeRF: Representing Scenes As Neural Radiance Fields for View Synthesis    | [Paper Link](https://arxiv.org/abs/2003.08934)             | 2020 | Introduces Neural Radiance Fields (NeRF) for synthesizing novel views of complex 3D scenes.                                                                           |                   |                                                           |
| 3   | When Does Label Smoothing Help?                                           | [Paper Link](https://arxiv.org/abs/1906.02629)             | 2019 | Explores the effectiveness of label smoothing in various training scenarios and its impact on model calibration.                                                      |                   |                                                           |
| 4   | Visualizing and Understanding Convolutional Networks                      | [Paper Link](https://arxiv.org/abs/1311.2901)              | 2013 | Provides techniques to visualize and understand the inner workings of Convolutional Neural Networks.                                                                  | [Summary_Readme.md](https://yourbloglink.com/post4)                  | [Blog](https://yourbloglink.com/post4)                     |
| 5   | Do vision transformers see like convolutional nets?                      | [Paper Link](https://arxiv.org/abs/2108.08810)             | 2021 | Compares the feature representations of Vision Transformers with Convolutional Neural Networks to understand their similarities and differences.                      |                   |                                                           |
| 6   | Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding | [Paper Link](https://arxiv.org/abs/2205.11487)             | 2022 | Discusses advanced diffusion models for generating photorealistic images from textual descriptions using deep language understanding.                                 |                   |                                                           |
| 7   | An Improved One Millisecond Mobile Backbone                               | [Paper Link](https://arxiv.org/abs/2206.04040)             | 2022 | Presents a highly efficient neural network architecture designed for mobile devices, achieving inference times of one millisecond.                                    |                   |                                                           |
| 8   | Neural Machine Translation                                                | [Paper Link](https://arxiv.org/abs/1409.1259)              | 2014 | Introduces a neural network model for machine translation that significantly improves translation quality by using large datasets and deep learning techniques.       |                   |                                                           |
| 9   | Attention is All You Need                                                 | [Paper Link](https://arxiv.org/abs/1706.03762)             | 2017 | Proposes the Transformer model, which relies entirely on attention mechanisms to boost performance in sequence transduction tasks.                                    |                   |                                                           |
| 10  | BERT                                                                      | [Paper Link](https://arxiv.org/abs/1810.04805)             | 2018 | Introduces BERT (Bidirectional Encoder Representations from Transformers), a model that pre-trains deep bidirectional representations by jointly conditioning on both left and right context in all layers. |                   |                                                           |
| 11  | Attention Interpretability Across NLP Tasks                               | [Paper Link](https://arxiv.org/abs/1909.11218)             | 2019 | Investigates the interpretability of attention mechanisms in various NLP tasks, providing insights into what models actually focus on.                                |                   |                                                           |
| 12  | Language Models are Few-Shot Learners                                     | [Paper Link](https://arxiv.org/abs/2005.14165)             | 2020 | Demonstrates that large-scale language models can perform well on a variety of tasks with few examples, introducing the concept of few-shot learning.                 |                   |                                                           |
| 13  | WebGPT                                                                    | [Paper Link](https://arxiv.org/abs/2112.09332)             | 2021 | Explores a version of GPT-3 fine-tuned to answer questions by searching the web, improving its ability to provide accurate and up-to-date information.                |                   |                                                           |
| 14  | PaLM                                                                      | [Paper Link](https://arxiv.org/abs/2204.02311)             | 2022 | Introduces PaLM (Pathways Language Model), a large-scale model designed to excel at language understanding and generation tasks through a unified architecture.       |                   |                                                           |
| 15  | LLaMA                                                                     | [Paper Link](https://arxiv.org/abs/2302.13971)             | 2023 | Proposes LLaMA, a series of foundation models that achieve competitive performance on a wide range of benchmarks with fewer parameters.                              |                   |                                                           |
| 16  | Chain of Verification                                                     | [Paper Link](https://arxiv.org/abs/2309.11495)             | 2023 | Introduces a novel approach to ensure the reliability of machine learning models by linking model outputs to verifiable evidence.                                     |                   |                                                           |
| 17  | Gemini                                                                    | [Paper Link](https://arxiv.org/abs/2312.11805)             | 2023 | Describes Gemini, a dual-pathway neural network model designed for better handling of multimodal data inputs.                                                        |                   |                                                           |
| 18  | How faithful are RAGs?                                                    | [Paper Link](https://arxiv.org/abs/2404.10198)             | 2024 | Evaluates the faithfulness of Retrieval-Augmented Generation (RAG) models in providing accurate and relevant information in generated outputs.                        |                   |                                                           |
| 19  | Distil-Whisper: Robust Knowledge Distillation Via Large-Scale Pseudo Labelling | [Paper Link](https://arxiv.org/abs/2311.00430)             | 2023 | Presents Distil-Whisper, a method for robust knowledge distillation using large-scale pseudo-labeling to enhance model performance.                                   |                   |                                                           |
| 20  | Modality Dropout for Improved Performance-driven Talking Faces            | [Paper Link](https://arxiv.org/abs/2005.13616)             | 2020 | Proposes a modality dropout technique to improve the performance of talking face generation models by selectively ignoring certain input modalities.                  |                   |                                                           |
| 21  | Pix to Video - Video Editing using image diffusion                        | [Paper Link](https://arxiv.org/abs/2303.12688)             | 2023 | Discusses a novel approach for video editing by applying image diffusion techniques to individual frames, ensuring temporal consistency.                             |                   |                                                           |
| 22  | Alpha-CLIP: a CLIP Model Focusing on Wherever You Want                    | [Paper Link](https://arxiv.org/abs/2312.03818)             | 2023 | Introduces Alpha-CLIP, an enhanced version of the CLIP model that allows for more precise control over the focus areas in generated images.                          |                   |                                                           |
| 23  | VideoPoet: a Large Language Model for Zero-Shot Video Generation          | [Paper Link](https://arxiv.org/abs/2312.14125)             | 2023 | Presents VideoPoet, a large language model capable of generating coherent videos from textual descriptions in a zero-shot manner.                                    |                   |                                                           |


## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

<!--
## Contributing

Contributions are welcome! Please read the [CONTRIBUTING.md](CONTRIBUTING.md) for details on the process for submitting pull requests.
-->

